# ETL Business Central a PostgreSQL

![ETL Pipeline](https://img.shields.io/badge/ETL-Pipeline-blue)
![Python](https://img.shields.io/badge/Python-3.12%2B-blue)
![SQLAlchemy](https://img.shields.io/badge/SQLAlchemy-PostgreSQL-orange)
![Pandas](https://img.shields.io/badge/Pandas-Data%20Handling-yellow)
![License](https://img.shields.io/badge/License-MIT-green)

Este proyecto implementa un **proceso ETL (Extract, Transform, Load)** diseÃ±ado para extraer datos de **Microsoft Dynamics 365 Business Central**, aplicar transformaciones y cargarlos en una base de datos **PostgreSQL**. La arquitectura sigue los principios de **Clean Architecture** y **SOLID** para promover la mantenibilidad, testabilidad y separaciÃ³n de responsabilidades.

---

## ğŸŒŸ CaracterÃ­sticas Principales

*   **ExtracciÃ³n desde Business Central:** Se conecta a la API de BC v2.0 usando OAuth2 (Client Credentials Flow).
*   **Carga en PostgreSQL:** Utiliza SQLAlchemy y `pandas.to_sql` para interactuar con la base de datos PostgreSQL.
*   **Arquitectura Limpia:** SeparaciÃ³n clara entre Dominio, AplicaciÃ³n, Adaptadores de Interfaz e Infraestructura.
*   **Pipeline Basado en Steps:** El flujo ETL se define como una secuencia de pasos (Steps) orquestados por un controlador, facilitando la extensiÃ³n.
*   **TransformaciÃ³n Centralizada:** La lÃ³gica de limpieza, filtrado y enriquecimiento de datos se ubica en servicios dedicados.
*   **InserciÃ³n Incremental:** Soporte para cargar datos en PostgreSQL evitando duplicados basados en una clave primaria definida.
*   **ConfiguraciÃ³n Flexible:** Uso de archivos `.env` para credenciales y `config.yaml` para otras configuraciones.
*   **Logging Integrado:** Registro detallado de eventos y errores durante la ejecuciÃ³n del pipeline.
*   **Tests Unitarios (con Pytest):** Incluye ejemplos de tests unitarios para validar componentes clave usando mocks.

---

## ğŸ—ï¸ Estructura del Proyecto

```plaintext
etl_bc/
â”œâ”€â”€ .env                   # Variables de entorno (Â¡NO versionar!)
â”œâ”€â”€ config.yaml            # ConfiguraciÃ³n adicional (ej. IDs excluidos)
â”œâ”€â”€ requirements.txt       # Dependencias Python
â”œâ”€â”€ config/
â”‚   â””â”€â”€ settings.py        # Carga .env y config.yaml, expone settings
â”œâ”€â”€ domain/
â”‚   â”œâ”€â”€ repositories/
â”‚   â”‚   â””â”€â”€ interfaces.py  # Interfaces para los repositorios (abstracciones)
â”‚   â””â”€â”€ services/
â”‚       â””â”€â”€ transform_service.py # LÃ³gica de transformaciÃ³n de datos
â”œâ”€â”€ infrastructure/
â”‚   â”œâ”€â”€ business_central/
â”‚   â”‚   â”œâ”€â”€ bc_client.py     # Cliente HTTP y autenticaciÃ³n con API BC
â”‚   â”‚   â””â”€â”€ bc_repository.py # ImplementaciÃ³n del repo BC usando el cliente
â”‚   â””â”€â”€ postgresql/
â”‚       â”œâ”€â”€ pg_client.py     # Gestiona el engine de SQLAlchemy
â”‚       â””â”€â”€ pg_repository.py # ImplementaciÃ³n del repo PG usando SQLAlchemy
â”œâ”€â”€ application/
â”‚   â””â”€â”€ use_cases/
â”‚       â”œâ”€â”€ bc_use_cases.py        # Orquesta llamadas a repos y servicios para lÃ³gica de BC
â”‚       â””â”€â”€ csv_export_service.py  # Servicio para exportar a CSV (opcional)
â”œâ”€â”€ interface_adapters/
â”‚   â””â”€â”€ controllers/
â”‚       â”œâ”€â”€ etl_controller.py      # Orquesta la ejecuciÃ³n del pipeline (lista de steps)
â”‚       â”œâ”€â”€ pipeline_extract.py    # Steps relacionados con la extracciÃ³n (E)
â”‚       â””â”€â”€ pipeline_store.py      # Steps relacionados con la carga (L) y conexiÃ³n
â”œâ”€â”€ tests/                   # Tests unitarios (pytest)
â”‚   â”œâ”€â”€ test_transform_service.py
â”‚   â”œâ”€â”€ test_bc_use_cases.py
â”‚   â””â”€â”€ test_pipeline_store.py
â”‚   â””â”€â”€ ...
â””â”€â”€ main.py                # Punto de entrada, configuraciÃ³n del pipeline e inyecciÃ³n de dependencias
```

## ğŸ“š DescripciÃ³n de Componentes Clave

*   **`config/settings.py` & `config.yaml`**: Gestionan toda la configuraciÃ³n. `settings.py` lee credenciales de `.env` y configuraciones generales de `config.yaml`. Se accede a travÃ©s de la instancia `settings`.
*   **`domain/`**: El nÃºcleo del negocio.
    *   **`repositories/interfaces.py`**: Define los "contratos" que la infraestructura debe cumplir (ej., quÃ© mÃ©todos debe tener un repositorio de BC).
    *   **`services/transform_service.py`**: Contiene la lÃ³gica pura de transformaciÃ³n (filtrar, limpiar, enriquecer datos), independiente de dÃ³nde vengan o a dÃ³nde vayan. Lee configuraciÃ³n especÃ­fica (como IDs a excluir) desde `settings`.
*   **`infrastructure/`**: Detalles tÃ©cnicos de cÃ³mo interactuar con sistemas externos.
    *   **`business_central/`**: Clases para hablar con la API de BC (`bc_client`) y la implementaciÃ³n concreta del repositorio (`bc_repository`) que usa el cliente.
    *   **`postgresql/`**: Clases para manejar la conexiÃ³n y operaciones con PostgreSQL usando SQLAlchemy (`pg_client`, `pg_repository`). `pg_repository` implementa mÃ©todos para crear tablas (con PK), insertar datos y realizar cargas incrementales.
*   **`application/use_cases/`**: Orquesta el flujo de datos entre repositorios y servicios de dominio para cumplir objetivos especÃ­ficos (ej., `get_companies` obtiene datos del repo BC y los pasa al `transform_service` para filtrar).
*   **`interface_adapters/controllers/`**: Adaptan las llamadas y orquestan el flujo general del ETL.
    *   **`etl_controller.py`**: Define la interfaz `ETLStepInterface` y la clase `ETLController` que ejecuta una lista de steps secuencialmente, pasando un diccionario de `context` entre ellos.
    *   **`pipeline_extract.py` / `pipeline_store.py`**: Contienen las implementaciones concretas de `ETLStepInterface` para cada paso del pipeline (Extraer CompaÃ±Ã­as, Extraer Proyectos, Verificar ConexiÃ³n PG, Almacenar Datos PG). Estos steps usan los `BCUseCases` y `PGRepository`.
*   **`main.py`**: Punto de entrada. Configura el logging, instancia todas las clases necesarias (clientes, repositorios, servicios, casos de uso, steps) e **inyecta las dependencias**. Define la secuencia de steps y lanza el `ETLController`.
*   **`tests/`**: Contiene los tests unitarios escritos con `pytest` y `unittest.mock` para verificar el comportamiento aislado de los servicios, casos de uso y steps.

---

## âš™ï¸ Requisitos Previos

*   **Python 3.10+** (recomendado 3.12+)
*   **pip** y **venv** (recomendado para entornos virtuales)
*   Acceso a un servidor **PostgreSQL** (local o remoto)
*   **Credenciales de AplicaciÃ³n Registrada en Azure AD** con permisos para la API de Business Central.

---

## ğŸš€ InstalaciÃ³n y ConfiguraciÃ³n

1.  **Clonar el repositorio:**
    ```bash
    git clone <url_del_repositorio>
    cd <nombre_del_repositorio>
    ```

2.  **(Recomendado) Crear y activar un entorno virtual:**
    ```bash
    python -m venv .venv
    # Linux/Mac
    source .venv/bin/activate
    # Windows (cmd/powershell)
    .venv\Scripts\activate
    ```

3.  **Instalar dependencias:**
    ```bash
    pip install -r requirements.txt
    ```
    *(AsegÃºrate de que `requirements.txt` incluya `requests`, `python-dotenv`, `pandas`, `SQLAlchemy`, `psycopg2-binary` (o `psycopg2`), `PyYAML`, `pytest`, `pytest-mock`)*

4.  **Configurar `.env`:**
    Crea un archivo `.env` en la raÃ­z del proyecto y aÃ±ade tus credenciales:
    ```dotenv
    # --- Business Central ---
    BC_TENANT_ID=TU_TENANT_ID_AZURE
    BC_CLIENT_ID=TU_CLIENT_ID_APLICACION
    BC_CLIENT_SECRET=TU_CLIENT_SECRET_APLICACION
    BC_ENVIRONMENT=production # O sandbox, etc.

    # --- PostgreSQL ---
    PG_HOST=localhost # O la IP/hostname de tu servidor PG
    PG_DBNAME=business_central # Nombre de la BD destino
    PG_USER=postgres # Usuario de la BD
    PG_PASSWORD=tu_password_segura
    PG_PORT=5432 # Puerto estÃ¡ndar de PG
    ```

5.  **Configurar `config.yaml`:**
    Crea/edita el archivo `config.yaml` (en la misma carpeta que `settings.py`, normalmente `config/`) para definir configuraciones como los IDs de compaÃ±Ã­as a excluir:
    ```yaml
    # config.yaml
    excluded_company_ids:
      - "ID_EMPRESA_A_EXCLUIR_1"
      - "ID_EMPRESA_A_EXCLUIR_2"
    # otras_configs: valor
    ```

---

## â–¶ï¸ EjecuciÃ³n

Una vez configurado, ejecuta el pipeline desde la raÃ­z del proyecto:

```bash
python main.py
```

El script realizarÃ¡ los siguientes pasos (segÃºn la configuraciÃ³n actual en `main.py`):

1.  **ConfigurarÃ¡ el logging.**
2.  **InstanciarÃ¡ las dependencias** necesarias (clientes, repositorios, servicios, casos de uso).
3.  **ExtraerÃ¡ CompaÃ±Ã­as:** ObtendrÃ¡ la lista de compaÃ±Ã­as desde Business Central y aplicarÃ¡ el filtrado definido en `config.yaml` a travÃ©s de `TransformService`. Opcionalmente, guardarÃ¡ las compaÃ±Ã­as *filtradas* en un archivo CSV (`companies_filtered_export.csv`).
4.  **ExtraerÃ¡ Proyectos:** ObtendrÃ¡ los datos de proyectos correspondientes Ãºnicamente a las compaÃ±Ã­as *filtradas* en el paso anterior.
5.  **VerificarÃ¡ ConexiÃ³n PG:** RealizarÃ¡ una prueba de conexiÃ³n a la base de datos PostgreSQL configurada.
6.  **AlmacenarÃ¡ CompaÃ±Ã­as:** UtilizarÃ¡ la lÃ³gica de inserciÃ³n incremental (`incremental_insert_table`):
    *   Si la tabla `companies_bc` no existe, la crearÃ¡ definiendo la columna `id` como Primary Key e insertarÃ¡ las filas Ãºnicas del DataFrame de compaÃ±Ã­as.
    *   Si la tabla `companies_bc` ya existe, consultarÃ¡ los `id` existentes e insertarÃ¡ Ãºnicamente las compaÃ±Ã­as del DataFrame actual cuyos `id` no se encuentren en la tabla.
7.  **AlmacenarÃ¡ Proyectos:** RealizarÃ¡ el mismo proceso incremental para la tabla `projects_bc`, usando tambiÃ©n la columna `id` como Primary Key.

*Observa la salida de la consola para ver los logs informativos (`INFO`, `DEBUG`, `WARNING`, `ERROR`) que detallan cada paso y cualquier problema encontrado.*

---

## ğŸ§ª EjecuciÃ³n de Tests

Para ejecutar los tests unitarios (asegÃºrate de haber instalado `pytest` y `pytest-mock`):

```bash
pytest
```
O para ejecutar los tests de un archivo especÃ­fico:
```bash
pytest tests/test_transform_service.py
```

# ğŸ§© Extensibilidad

El diseÃ±o basado en Steps permite aÃ±adir fÃ¡cilmente nueva funcionalidad al pipeline:

1. **Define la LÃ³gica:**  
   Implementa la nueva funcionalidad de extracciÃ³n, transformaciÃ³n o carga dentro de las capas apropiadas (Repositorio, Servicio, Caso de Uso).

2. **Crea un Nuevo Step:**  
   Define una nueva clase en `pipeline_extract.py`, `pipeline_store.py` (o crea un `pipeline_transform.py`) que herede de `ETLStepInterface`.

3. **Implementa `run(self, context)`:**  
   Escribe el cÃ³digo dentro del mÃ©todo `run` de tu nuevo Step. Llama a los Casos de Uso necesarios, lee/escribe en el diccionario `context` y devuelve el `context` actualizado.

4. **AÃ±ade el Step a `main.py`:**  
   Instancia tu nuevo step en `main.py` y aÃ±Ã¡delo a la lista `steps` en la posiciÃ³n deseada dentro de la secuencia del pipeline.

# ğŸ“œ Principios y Patrones

- **Clean Architecture / SOLID:**  
  Se busca separar las preocupaciones (negocio, aplicaciÃ³n, infraestructura), reducir el acoplamiento y aumentar la cohesiÃ³n, facilitando el mantenimiento y la evoluciÃ³n del cÃ³digo.

- **InyecciÃ³n de Dependencias:**  
  Las dependencias (objetos que una clase necesita para funcionar) se crean en el punto de entrada (`main.py`) y se "inyectan" a las clases a travÃ©s de sus constructores. Esto mejora la testabilidad y flexibilidad.

- **PatrÃ³n Repositorio:**  
  Abstrae los detalles del acceso a los datos (cÃ³mo hablar con la API de BC o con la BD PG), proporcionando una interfaz limpia a la capa de aplicaciÃ³n.

- **PatrÃ³n Servicio:**  
  Encapsula lÃ³gica de negocio reutilizable o procesos de transformaciÃ³n complejos (por ejemplo, `TransformService`).

- **Pipeline (Steps + Controller):**  
  Modela el proceso ETL como una cadena de pasos discretos y bien definidos, orquestados por un controlador (`ETLController`), lo que facilita la comprensiÃ³n y modificaciÃ³n del flujo.
